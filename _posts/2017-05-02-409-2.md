---
id: 409
title: 'Feature Scaling in Machine Learning: A Practical Guide'
date: '2017-05-02T18:47:16+00:00'
author: 'Gian Paolo'
layout: post
guid: 'https://genmind.ch/?p=409'
permalink: /409-2/
site-sidebar-layout:
    - default
ast-site-content-layout:
    - default
site-content-style:
    - default
site-sidebar-style:
    - default
theme-transparent-header-meta:
    - default
astra-migrate-meta-layouts:
    - set
image: /wp-content/uploads/2017/05/scaling.jpg
categories:
    - 'data science'
---

Feature Scaling in Machine Learning: A Practical Guide  
When training machine learning models, having features on vastly different scales can slow down learning or even lead to suboptimal performance. Feature scaling‚Äîoften simply called normalization‚Äîis a technique to adjust the range of your features so that they become comparable. In this post, we‚Äôll explain a few common methods, including simple scaling, mean normalization, and Z‚Äëscore normalization, and show you how to implement them in Python.

Why Scale Your Features?  
Imagine one feature represents house sizes in square feet (ranging from 300 to 2,000) while another captures the number of bedrooms (ranging from 0 to 5). Without scaling, the larger values of the house size could dominate the learning process‚Äîeven if both features are equally important. Scaling transforms each feature so that they contribute equally, which can also help gradient descent converge faster.

Methods for Feature Scaling  
1\. Simple Scaling by Maximum Value  
One straightforward method is to divide each feature by its maximum value. For example, if your feature x1 ranges from 300 to 2,000, you can obtain a scaled version by calculating:

$$  
x\_{1,\\text{scaled}} = \\frac{x\_1}{x\_{1,\\text{max}}}  
$$

This operation transforms ùë•1 so that its values now lie roughly between 0.15 and 1. Likewise, if another feature ùë•2 ranges from 0 to 5, dividing by 5 will re-scale it to the  
\[0,1\] interval.

Below is a simple Python example using scikit-learn:

```

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Example dataset
data = pd.DataFrame({
    'HouseSize': [300, 600, 1200, 1800, 2000],
    'Bedrooms': [1, 2, 3, 4, 5]
})

# Initialize the scaler for each feature (using max values)
scaler = MinMaxScaler(feature_range=(0, 1))

# Fit and transform the dataset
data_scaled = scaler.fit_transform(data)

# Convert back to a DataFrame for readability
scaled_df = pd.DataFrame(data_scaled, columns=['HouseSize_scaled', 'Bedrooms_scaled'])
print(scaled_df)

```

In this code, each column is scaled so that its values fall between 0 and 1.

2\. Mean Normalization  
Mean normalization not only scales the values but also centers them around zero. For feature ùë•1, if the mean ùúá1 is 600, you calculate:  
$$  
x\_{1,\\text{norm}} = \\frac{x\_1 ‚Äì \\mu\_1}{x\_{1,\\text{max}} ‚Äì x\_{1,\\text{min}}}  
$$

‚Äã

This results in values that might range from approximately ‚Äì0.18 to 0.82.

Here‚Äôs a Python example to perform mean normalization manually:

```

import numpy as np

# Original feature values for House Size
house_sizes = np.array([300, 600, 1200, 1800, 2000])

# Calculate mean, min, and max
mu = np.mean(house_sizes)
min_val = np.min(house_sizes)
max_val = np.max(house_sizes)

# Perform mean normalization
house_sizes_norm = (house_sizes - mu) / (max_val - min_val)
print("Mean Normalized House Sizes:", house_sizes_norm)

```

You can apply a similar transformation to any feature to center it around zero.

3\. Z‚ÄëScore Normalization  
Z‚Äëscore normalization (or standardization) transforms data by subtracting the mean and dividing by the standard deviation. For a feature ùë•1 with mean ùúá1 and standard deviation ùúé1, each value is transformed as:

$$  
x\_{1,\\text{z}} = \\frac{x\_1 ‚Äì \\mu\_1}{\\sigma\_1}  
$$

This standardizes x1 so that it has a mean of 0 and a standard deviation of 1.

Below is a Python example using scikit-learn‚Äôs StandardScaler:

```

from sklearn.preprocessing import StandardScaler

# Example dataset (same house sizes)
house_sizes = np.array([[300], [600], [1200], [1800], [2000]])

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the data
house_sizes_standardized = scaler.fit_transform(house_sizes)

print("Z-Score Normalized House Sizes:")
print(house_sizes_standardized)


```

Visualizing Scaled Data  
When you plot your scaled features (whether using simple division, mean normalization, or Z‚Äëscore normalization), you‚Äôll notice that the data points become more comparable. A typical rule of thumb is to aim for a feature range of roughly ‚Äì1 to +1. However, small deviations‚Äîsuch as ‚Äì0.3 to +0.3 or even ‚Äì3 to +3‚Äîare perfectly acceptable depending on your application.

Final Thoughts  
Feature scaling is an almost universal preprocessing step that can improve the performance of many machine learning algorithms‚Äîespecially those sensitive to the magnitudes of input values like gradient descent and distance-based models. Whether you choose simple scaling, mean normalization, or Z‚Äëscore normalization depends on your data‚Äôs characteristics and the specific needs of your model.

By ensuring that each feature contributes fairly, you pave the way for faster convergence and a more robust model. In practice, when in doubt, scaling your features rarely hurts‚Äîand may even significantly enhance your model‚Äôs performance.

Happy scaling!