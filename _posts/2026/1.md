
*******************From Recurrence (RNN) to Attention-Based NLP Models
00:04 - SPEAKER: So today, we've got what
00:07 - I think is quite an exciting lecture topic.
00:09 - We're going to talk about self-attention
00:11 - and transformers.
00:13 - So these are some ideas that are sort of the foundation of most
00:18 - of the modern advances in natural language processing
00:21 - and actually sort of AI systems in a broad range of fields.
00:26 - So it's a very, very fun topic.
00:28 - OK.
00:30 - So let's kind of take a look back
00:32 - into what we've done so far in this course and sort of
00:36 - see what we were doing in natural language processing.
00:40 - What was our strategy?
00:41 - If you had a natural language processing problem
00:43 - and you wanted to say take like your best effort attempt at it
00:46 - without doing anything too fancy,
00:48 - you would have said, OK, I'm going
00:49 - to have a bidirectional LSTM instead of a simple RNN.
00:54 - I'm going to use an LSTM to encode my sentences.
00:57 - I get bidirectional context.
00:59 - And if I have an output that I'm trying to generate, right?
01:02 - I'll have a unidirectional LSTM that I was
01:06 - going to generate one by one.
01:07 - So you have a translation or a parse or whatever.
01:10 - And so maybe I've encoded in a bidirectional LSTM, the source
01:13 - sentence, and I'm sort of one by one
01:16 - decoding out the target with my unidirectional LSTM.
01:19 - And then also, I was going to use something like attention
01:23 - to give flexible access to memory if I felt like I needed
01:28 - to do this sort of look back and see
01:30 - where I want to translate from.
01:32 - OK, and this was just working exceptionally well,
01:35 - and we motivated attention through wanting
01:38 - to do machine translation, and you have this bottleneck
01:41 - where you don't want to have to encode the whole source
01:43 - sentence in a single vector.
01:45 - OK, and in this lecture, we have the same goal.
01:48 - So we're going to be looking at a lot of the same problems
01:50 - that we did previously, but we're
01:52 - going to use different building blocks.
01:53 - So we're going to say if 2014 to 2017-ish I was using recurrence
02:00 - through lots of trial and error--
02:02 - years later, we had these brand new building blocks that we
02:06 - could plug in sort of direct replacement for LSTMs,
02:10 - and they're going to allow for just a huge range of much more
02:15 - successful applications.
02:18 - And so what are the issues with recurrent neural networks
02:22 - we used to use?
02:22 - And what are the new systems that we're
02:24 - going to use from this point moving forward?
02:26 -
02:29 - So one of the issues with a recurrent neural network
02:32 - is what we're going to call linear interaction distance.
02:35 - So as we know, RNNs are unrolled left to right or right to left,
02:40 - depending on the language and the direction.
02:43 - OK, but it encodes this sort of notion
02:44 - of linear locality, which is useful
02:46 - because if two words occur right next to each other,
02:48 - sometimes they're actually quite related.
02:50 - So tasty pizza.
02:52 - They're nearby.
02:53 - And in the recurrent neural network,
02:55 - you sort of encode tasty, and then you sort of walk one step,
02:59 - and you encode pizza.
03:02 - So nearby words do often affect each other's meanings,
03:06 - but you have this problem where very long distance dependencies
03:10 - can take a very long time to interact.
03:12 - So if I have this sentence "the chef"--
03:14 - so those are nearby.
03:16 - Those interact with each other.
03:18 - And then "who," and then a bunch of stuff,
03:22 - like the chef who went to the stores
03:24 - and picked up the ingredients and, you know, loves garlic.
03:28 - And then "was" like I actually have an RNN step.
03:33 - This sort of application of the recurrent weight matrix
03:36 - and some element-wise non-linearities once, twice,
03:39 - three times sort of as many times as there is potentially
03:43 - the length of the sequence between chef and what's.
03:47 - And it's "the chef who was."
03:48 - So this is a long-distance dependency
03:50 - should feel kind of related to the stuff
03:52 - that we did in dependency syntax.
03:54 - But it's quite difficult to learn potentially
03:59 - that these words should be related.
04:02 - So if you have a lot of steps between words,
04:12 - it can be difficult to learn the dependencies between them.
04:15 - We talked about all these gradient problems.
04:17 - LSTMs do a lot better at modeling the gradients
04:21 - across long distances than simple recurrent neural
04:24 - networks, but it's not perfect.
04:27 - And we already know that this linear order
04:30 - isn't the right way to think about sentences.
04:34 - So if I wanted to learn that it's the chef who was,
04:40 - then I might have a hard time doing it
04:43 - because the gradients have to propagate from was to chef
04:46 - and really, I'd like more direct connection between words
04:50 - that might be related in the sentence or in a document
04:53 - even if these are going to get much longer.
04:57 - So this linear interaction distance problem.
04:59 - We would like words that might be related
05:01 - to be able to interact with each other in the neural networks
05:04 - computation sort of graph more easily than sort of being
05:10 - linearly far away so that we can learn
05:14 - these long-distance dependencies better.
05:16 - And there's a related problem too
05:17 - that again comes back to the recurrent neural network's
05:20 - dependence on the index-- on the index
05:22 - into the sequence, often called a dependence on time.
05:26 - So in a recurrent neural network the forward and backward passes
05:30 - have O of sequence length many.
05:33 - So that means just roughly sequence,
05:34 - in this case, just sequence length
05:36 - many unparallelizable operations.
05:38 - So we know GPUs are great.
05:40 - They can do a lot of operations at
05:43 - once as long as there's no dependency
05:45 - between the operations in terms of time,
05:47 - that you have to compute one and then compute the other.
05:51 - But in a recurrent neural network,
05:53 - you can't actually compute the RNN hidden state for time step
05:56 - 5 before you compute the RNN hidden state for time step 4
06:00 - or time step 3, right?
06:02 - And so you get this graph that looks very similar
06:04 - where if I want to compute this hidden state.
06:06 - So I've got some word--
06:08 - I have zero operations I need to do before I
06:10 - can compute this state.
06:11 - I have one operation I can do before I
06:13 - can compute this state.
06:15 - And as my sequence length grows, I've got--
06:18 - OK, here I've got three operations
06:20 - I need to do before I can compute
06:22 - the state with the number 3 because I need to compute
06:24 - this and this and that.
06:27 - So there's sort of three unparallelizable operations
06:30 - that I'm sort of glomming all the matrix multiplies
06:32 - and stuff into a single one.
06:34 - So 1, 2, 3--
06:35 - and of course, this grows with the sequence length as well.
06:38 - So down over here, as the sequence length grows,
06:42 - I can't parallelize--
06:44 - I can't just have a big GPU just kachunka with the matrix
06:48 - multiply to compute this state because I need to compute all
06:51 - the previous states beforehand.
06:53 - So the question is, with the linear interaction distance,
06:56 - wasn't this the point of attention
06:57 - that it sort of gets around that?
06:59 - Can't we use something with attention to sort of help,
07:01 - or does that just help?
07:02 - So it won't solve the parallelizability problem.
07:04 - And, in fact, everything we do in the rest of this lecture
07:07 - will be attention-based, but we'll get rid of the recurrence
07:09 - and just do attention more or less.
07:11 - So yeah, it's a great intuition.
07:14 - So if not recurrence what about attention.
07:16 - So you just a slide-- a slide back.
07:18 - And so just we're going to get deep into attention today,
07:22 - but just for the second.
07:24 - Attention treats each word's representation
07:26 - as a query to access and incorporate information
07:29 - from a set of values.
07:31 - So previously, we were in a decoder.
07:33 - We were decoding out a translation of a sentence,
07:35 - and we attended to the encoder so
07:37 - that we didn't have to store the entire representation
07:39 - of the source sentence into a single vector.
07:42 - And here today, we'll think about attention
07:44 - within a single sentence.
07:45 - So I've got this sort of sentence written out here
07:48 - with a word 1 through word T in this case.
07:51 - And right on these sort of integers in the boxes,
07:54 - I'm writing out the number of parallelizable operations
07:57 - that you need to do before you can compute these.
08:00 - So for each word, you can independently
08:02 - compute its embedding without doing anything else previously,
08:04 - right, because the embedding just
08:06 - depends on the word identity.
08:08 - And then with attention--
08:11 - if I wanted to build an attention representation
08:13 - of this word by looking at all the other words,
08:15 - in the sequence, that's sort of one big operation.
08:18 - And I can do them in parallel for all the words.
08:20 - So the attention for this word, I
08:22 - can do for the attention for this word.
08:24 - I don't need to walk left to right like I did for an RNN.
08:27 - Again, we'll get much deeper into this,
08:29 - but this you should have the intuition
08:32 - that it solves the linear interaction
08:34 - problem and the non-parallelizability problem
08:37 - because now no matter how far away words are from each other,
08:40 - I am potentially interacting.
08:42 - I might just attend to you even if you're very, very far away,
08:46 - sort of independent of how far away you are.
08:48 - And I also don't need to sort of walk along
08:51 - the sequence linearly long.
08:52 - So I'm treating the whole sequence at once.
08:54 - All right.
08:56 - So the intuition is that attention
08:59 - allows you to look very far away at once,
09:00 - and it doesn't have this dependence on the sequence
09:03 - index that keeps us from parallelizing operations.
09:05 - And so now the rest of the lecture,
09:07 - we'll talk in great depth about attention.
09:10 - So maybe let's just move on.
09:13 - OK.
09:14 - So let's think more deeply about attention.
09:18 - One thing that you might think of with attention
09:21 - is that it's sort of performing kind of a fuzzy
09:23 - lookup in a key value store.
09:25 - So you have a bunch of keys, a bunch of values,
09:27 - and it's going to help you access that.
09:30 - So in an actual lookup table, right, just like a dictionary
09:34 - in Python, for example, right?
09:35 - Very simple.
09:36 - You have a table of keys that each key maps to a value,
09:40 - and then you give it a query, and the query
09:42 - matches one of the keys.
09:44 - And then you return the value, right?
09:46 - So I've got a bunch of keys here,
09:49 - and my query matches the key.
09:51 - So I return the value.
09:53 - Simple, fair, easy.
09:55 - OK.
09:56 - Good.
09:57 - And in attention, just like we saw before,
10:02 - the query matches all keys softly.
10:05 - There's no exact match.
10:07 - You sort of compute some sort of similarity between the key
10:10 - and all of the-- sorry, the query and all of the keys.
10:13 - And then you sort of weigh the results.
10:14 - So you've got a query again.
10:16 - You've got a bunch of keys.
10:18 - The query to different extents is similar to each of the keys,
10:22 - and you will sort of measure that similarity between 0 and 1
10:26 - through a softmax.
10:28 - And then you get the values out.
10:30 - You average them via the weights of the similarity
10:34 - between the key and the query and the keys.
10:36 - You do a weighted sum with those weights, and you get an output.
10:40 - So it really is quite a bit like a lookup table
10:43 - but in this sort of soft vector space,
10:45 - you know, mushy sort of sense.
10:48 - So I'm really doing some kind of accessing
10:50 - into this information that's stored in the key value store,
10:54 - but I'm sort of softly looking at all of the results.
10:57 - So what might this look like?
10:58 - So if I was trying to represent this sentence,
11:01 - I went to Stanford CS 224n and learned.
11:04 - So I'm trying to build a representation of learned.
11:06 -
11:10 - I have a key for each word.
11:12 - So this self-attention thing that we'll get into.
11:14 - I have a key for each word, a value for each word.
11:17 - I've got the query for learned, and I've got these sort
11:20 - of teal-ish bars up top, which sort of might say how much
11:23 - you're going to try to access each of the word like so maybe
11:26 - 224n is not that important CS, maybe that determines what I
11:30 - learned--
11:31 - Stanford.
11:33 - And then learned maybe that's important for representing
11:35 - itself.
11:36 - So you look across at the whole sentence
11:38 - and build up this sort of soft accessing of information
11:41 - across the sentence in order to represent learned in context.
11:46 - OK, so this is just a toy diagram.
11:49 - So let's get into the math.
11:50 - So we're going to look at a sequence of words,
11:53 - that's W1 to n--
11:55 - sequence of words in a vocabulary.
11:57 - So this is like Zuko made his uncle tea.
11:59 - That's a good sequence.
12:00 - And for each word, we're going to embed it
12:02 - with this embedding matrix just like we've
12:04 - been doing in this class.
12:06 - So I have this embedding matrix that goes from the vocabulary
12:09 - size to the dimensionality d.
12:12 - So that's each word has a non-contextual--
12:15 - only dependent on itself word embedding.
12:17 - And now I'm going to transform each word with one of three
12:21 - different weight matrices.
12:22 - So this is often called key query value self-attention,
12:26 - right?
12:27 - So I have a matrix q, which is an Rd to d.
12:30 - So this maps xi to which is a vector of dimensionality d
12:33 - to another vector of dimensionality d.
12:35 - And so that's going to be a query vector, right?
12:38 - So it takes an xi, and it sort of rotates it, shuffles it
12:42 - around, stretches it, squishes it, makes it different.
12:45 - And now it's a query.
12:46 - And now for a different learnable parameter k.
12:48 - So that's another matrix.
12:50 - So I'm going to come up with my keys.
12:52 - And with a different learnable parameter v,
12:55 - I'm going to come up with my values, right?
12:57 - So I'm taking each of the non-contextual word
12:59 - embeddings each of these xis, and I'm
13:01 - transforming each of them to come up
13:04 - with my query for that word, my key for that word,
13:07 - and my value for that word.
13:10 - So every word is doing each of these roles.
13:14 - Next, I'm going to compute all pairs of similarities
13:17 - between the keys and queries.
13:18 - So in the toy example we saw, I was computing the similarity
13:22 - between a single query for the word learned and all
13:25 - of the keys for the entire sentence.
13:27 - In this context, I'm computing all pairs of similarities
13:30 - between all keys and all values because I
13:33 - want to represent sort of all of these sums.
13:35 - So I've got this sort of dot product--
13:37 - I'm just going to take the dot product between these two
13:40 - vectors, right?
13:40 - So I've got qi.
13:42 - So this is saying the query for word i dotted with the key
13:45 - for word j, and I get this score, which
13:48 - is a real value, might be a very large negative, might be zero,
13:53 - might be very large and positive.
13:54 - And so that's like how much should i
13:57 - look at j in this lookup table.
14:00 - And then I do the softmax, right?
14:01 - So I softmax.
14:02 - So I say that the actual weight that I'm
14:05 - going to look at j from i is softmax of this over all
14:09 - of the possible indices, right?
14:11 - So it's like the affinity between i
14:13 - and j normalized by the affinity between i
14:16 - and all of the possible j prime in the sequence.
14:21 - And then my output is just the weighted sum of values.
14:24 - So I've got this output for word i.
14:26 - So maybe i is like one for Zuko, and I'm
14:29 - representing it as the sum of these weights for all j.
14:33 - So Zuko and made and his and uncle and tea,
14:36 - and the value vector for that word j.
14:40 - I'm looking from i to j as much as alpha ij.
14:44 - Oh, Wi, you can either think of it
14:46 - as a symbol in vocab v. So that's
14:50 - like you could think of it as a one hot vector in--
14:54 - yeah, in this case, we are, I guess, thinking of this.
14:56 - So a one hot vector in dimensionality size of vocab.
14:59 - So in the matrix E, you see that it's
15:02 - Rd by bars around v. That's size of the vocabulary.
15:07 - So when I do E multiplied by Wi, that's
15:10 - taking E, which is d by v, multiplying it
15:14 - by W, which is v, and returning a vector
15:16 - that's dimensionality d.
15:18 - Yeah, it's n, which is the sequence length.
15:20 - And then the second dimension would
15:22 - be v, which is the vocabulary size.
15:24 - And then that gets mapped to this thing, which
15:26 - is sequence length by d.
15:28 - So this eii, right, for j equal to i, so looking at itself.
15:33 - Looks like anything in particular.
15:34 - Does it look like the identity?
15:36 - Is that the question--
15:37 - OK, so right, it's unclear actually.
15:41 - This question of should you look at yourself
15:43 - for representing yourself, well, it's
15:45 - going to be encoded by the matrices Q and K.
15:49 - If I didn't have Q and K in there--
15:51 - if those were the identity matrices-- if Q is identity,
15:54 - K is identity, then this would be
15:55 - sort of dot product with yourself, which
15:57 - is going to be high on average like you're pointing
16:00 - in the same direction as yourself
16:02 - but it could be that Qxi and Kxi might be sort of arbitrarily
16:08 - different from each other because Q could be the identity
16:12 - and K could map you to the negative of yourself,
16:15 - for example, so that you don't look at yourself.
16:17 - So this is all learned in practice.
16:19 - So you end up--
16:20 - it can sort of decide by learning whether you should
16:25 - be looking at yourself or not.
16:26 - And that's some of the flexibility that parametrizing
16:28 - it as Q and K gives you that wouldn't be there
16:32 - if I just used xis.
16:34 - So this is our basic building block,
16:37 - but there are a bunch of barriers to using it
16:40 - as a replacement for LSTMs.
16:43 - And so what we're going to do for this portion of the lecture
16:45 - is talk about the minimal components
16:47 - that we need in order to use self-attention
16:50 - as sort of this like very fundamental building block.
16:54 - So we can't use it as it stands, as I've presented it
16:57 - but because there are a couple of things that we
16:59 - need to sort of solve or fix.
17:01 - One of them is that there's no notion of sequence
17:04 - order in self-attention.
17:06 -
17:08 - What does this mean?
17:10 - If I have a sentence like "Zuko made his uncle,"
17:20 - and let's say "his uncle made Zuko."
17:31 - If I were to embed each of these words,
17:32 - right, using its embedding matrix--
17:35 - the embedding matrix isn't dependent on the index
17:39 - of the word.
17:40 - So this is the word at index 1, 2, 3, 4 versus his
17:44 - is over here and uncle.
17:47 - And so when I compute the self-attention,
17:49 - and there's a lot more on this in the lecture notes that
17:50 - goes through a full example.
17:51 -
17:54 - The actual self-attention operation will give you exactly
17:58 - the same representations for this sequence--
18:00 - Zuko made his uncle-- as for this sequence--
18:03 - his uncle made Zuko.
18:05 - And that's bad because they're sentences
18:06 - that mean different things.
18:08 - And so it's sort of this idea that self-attention
18:12 - is an operation on sets like you have a set of vectors
18:15 - that you're going to perform self-attention on.
18:17 - And nowhere does like the exact position of the words come
18:21 - into play directly.
18:24 - So we're going to encode the position of words
18:27 - through the keys, queries, and values that we have.
18:31 - So consider now representing each sequence index--
18:35 - our sequences are going from 1 to n-- as a vector.
18:39 - So don't worry so far about how it's being made,
18:42 - but you can imagine representing sort of the number one
18:45 - like the position 1, the position 2, the position 3
18:48 - as a vector in the dimensionality
18:50 - d just like we're representing our keys, queries, and values.
18:54 - And so these are position vectors.
18:57 - You can-- if you were to want to incorporate the information
19:03 - represented by these positions into our self-attention,
19:07 - you could just add these vectors--
19:09 - these pi vectors to the inputs.
19:13 - So if I have this xi embedding of a word which
19:18 - is the word at position i but really just represents the word
19:21 - "Zuko is here," now I can say that, oh, it's the word "Zuko,"
19:25 - and it's at position 5 because this vector represents
19:29 - position 5.
19:29 -
19:33 - OK, so how do we do this?
19:36 - And we might only have to do this once.
19:37 - So we can do it once at the very input to the network,
19:41 - and then that is sufficient.
19:43 - We don't have to do it at every layer
19:45 - because it sort of knows from the input.
19:48 - So one way in which people have done this
19:51 - is look at these sinusoidal position representations.
19:54 - So this looks a little bit like this, where you have--
19:57 - so these are-- this is a vector pi which is in dimensionality
20:00 - d, right, and each one of the dimensions you take the value i
20:05 - you modify it by some constant, and you pass it to the sine
20:11 - or cosine function, and you get these sort of values that vary
20:15 - according to the period--
20:17 - differing periods depending on the dimensionality d.
20:19 - So I've got this sort of a representation
20:21 - of a matrix, where d is the vertical dimension
20:24 - and then n is the horizontal.
20:26 - And you can see that there's sort of like, oh,
20:30 - as I walk along, you see the period of the sine function
20:32 - going up and down, and each of the dimensions
20:34 - d has a different period.
20:36 - And so together, you can represent
20:37 - a bunch of different sort of position indices.
20:40 - And it gives this intuition that, oh,
20:44 - maybe sort of absolute position of a word isn't as important.
20:47 - You've got this sort of periodicity of the sines
20:49 - and cosines, and maybe that allows you to extrapolate
20:53 - to longer sequences.
20:54 - But in practice, that doesn't work.
20:57 - But this is sort of like an early notion
20:59 - that is still sometimes used for how to represent position
21:02 - in transformers and self-attention networks
21:05 - in general.
21:08 - So that's one idea.
21:10 - You might think it's a little bit complicated,
21:13 - a little bit unintuitive.
21:15 - Here's something that feels a little bit more deep learning.
21:19 - So we're just going to say, oh, you know,
21:22 - I've got a maximum sequence length of n.
21:24 - And I'm just going to learn a matrix that's dimensionality
21:28 - d by n, and that's going to represent my position.
21:30 - And I'm going to learn it as a parameter,
21:32 - just like I learn every other parameter.
21:34 - And what do they mean?
21:35 - Oh, I have no idea, but it represents position.
21:37 -
21:41 - So you just sort of add this matrix to the xi.
21:44 - So your input embeddings, and it learns to fit to data.
21:49 - So whatever representation of position
21:51 - that's linear sort of index based that you want you
21:55 - can learn.
21:56 - And the cons are that, well, you definitely now
21:59 - can't represent anything that's longer than n words long.
22:03 - No sequence longer than n you can handle because, well, you
22:07 - only learned a matrix of this many positions.
22:09 - And so, in practice, you'll get a model error
22:12 - if you pass a self-attention model.
22:15 - Something longer than length n, it will just sort of crash
22:18 - and say I can't do this.
22:21 - And so this is what most systems nowadays use.
22:24 - There are more flexible representations of position,
22:27 - including a couple in the lecture notes.
22:29 - You might want to look at sort if
22:31 - like the relative linear position or words
22:33 - before or after each other but not their absolute position.
22:36 - There's also some sort of representations
22:38 - that hearken back to our dependency syntax
22:41 - because like, oh, maybe words that
22:42 - are close in the dependency parse tree
22:44 - should be the things that are sort
22:45 - of close in the self-attention operation.
22:49 - So the question is, in practice, do we just
22:51 - make n long enough that we don't run into the problem
22:53 - where we're going to look at a text longer than n?
22:56 - No.
22:57 - In practice, it's actually quite a problem.
22:59 - Even today, even in the largest, biggest language models and can
23:05 - I fit this prompt into ChatGPT or whatever is the thing
23:08 - that you might see on Twitter?
23:10 - I mean, these continue to be issues, and part of it
23:12 - is because the self-attention operation,
23:15 - and we'll get into this later in the lecture.
23:17 - It's quadratic complexity in the sequence length.
23:20 - So you're going to spend n squared sort of memory budget
23:24 - in order to make sequence lengths longer.
23:26 - So in practice, this might be on a large model,
23:29 - say, 4,000 or so.
23:31 - And is 4,000, so you can fit 4,000 words, which
23:34 - feels like a lot, but it's not going to fit a novel.
23:36 - It's not going to fit a Wikipedia page.
23:39 - You know and so-- and there are models that
23:41 - do longer sequences for sure.
23:44 - And again, we'll talk a bit about it
23:45 - but know this actually is an issue.
23:47 - So how do you know that the p that you've
23:49 - learned, this matrix that you've learned
23:51 - is representing position as opposed to anything else?
23:53 - And the reason is the only thing it correlates
23:55 - is position, right?
23:56 - So like when I see these vectors,
23:58 - I'm adding this p matrix to my x matrix, the word embeddings.
24:03 - I'm adding them together, and the words
24:05 - that show up at each index will vary depending
24:07 - on what word actually showed up there in the example,
24:10 - but the p matrix never differs.
24:11 - It's always exactly the same at every index.
24:14 - And so it's the only thing in the data
24:16 - that it correlates with.
24:17 - So you're sort of learning it implicitly.
24:19 - You like this vector at index 1 is always
24:20 - at index 1 for every example for every gradient update,
24:24 - and nothing else co-occurs like that.
24:29 - Yeah, so what you end up learning--
24:31 - I don't know, it's unclear, but it definitely
24:33 - allows you to know, oh, this word is
24:36 - with this index at this-- yeah.
24:37 - So the question is, when this is quadratic in the sequence,
24:40 - is that a sequence of words?
24:41 - Yeah, think of it as a sequence of words.
24:43 - Sometimes there'll be pieces that
24:45 - are smaller than words, which we'll
24:46 - go into in the next lecture.
24:48 - But yeah, think of this as a sequence of words,
24:50 - but not necessarily just for a sentence.
24:52 - Maybe for an entire paragraph or an entire document or something
24:56 - like that.
24:57 - OK, so right-- so we have another problem.
25:00 - Another is that based on the presentation of self-attention
25:04 - that we've done, there's really no non-linearities for sort
25:08 - of deep learning magic.
25:09 - We're just sort of computing weighted averages of stuff.
25:14 - So if I apply self-attention and then apply self-attention
25:18 - again and then again and again and again, you
25:21 - shouldn't get-- you should look at the lecture
25:22 - notes if you're interested in this.
25:23 - It's actually quite cool.
25:24 - But what you end up doing is you're just reaveraging
25:27 - value vectors together.
25:28 - So you're like computing averages of value vectors,
25:31 - and it ends up looking like one big self-attention.
25:33 - But there's an easy fix to this if you
25:35 - want the traditional deep learning magic,
25:38 - and you can just add a feedforward network
25:40 - to post-process each output vector.
25:42 - So I've got a word here.
25:43 - That's the output of self-attention,
25:45 - and I'm going to pass it through--
25:47 - in this case, I'm calling it a multi-layer perceptron--
25:50 - MLP.
25:50 - So this is a vector in RD that's going to be--
25:53 - and it's taking in as input a vector in RD.
25:56 - And you do the usual sort of multi-layer perceptron thing,
26:00 - where you have the output and you multiply it by a matrix,
26:02 - pass it through a non-linearity, multiply it by another matrix.
26:06 - And so what this looks like in self-attention
26:08 - is that I've got this sentence, the chef who dot,
26:11 - dot, dot, dot, the food, and I've got my embeddings for it.
26:14 - I pass it through this whole big self-attention block
26:16 - which looks at the whole sequence
26:18 - and sort of incorporates context and all that.
26:20 - And then I pass each one individually
26:23 - through a feedforward layer.
26:25 - So this embedding that's the output of the self-attention
26:28 - for the word "the" is passed independently
26:31 - through a multi-layer perceptron here, and that sort of-- you
26:34 - can think of it as sort of combining
26:37 - together or processing the result of attention.
26:41 - So there's a number of reasons why we do this.
26:44 - One of them also is that you can actually
26:46 - stack a ton of computation into these feedforward networks
26:50 - very, very efficiently, very parallelizable, very good
26:53 - for GPUs, but this is what's done in practice.
26:56 - So you do self-attention and then
26:57 - you can pass it through this sort
26:59 - of position-wise feedforward layer.
27:02 - Every word is processed independently
27:04 - by this feedforward network to process the result.
27:10 - OK, so that's adding our sort of classical deep learning
27:13 - non-linearities for self-attention.
27:16 - And that's an easy fix for this sort
27:18 - of no non-linearities problem in self-attention.
27:21 - And then we have a last issue before we
27:23 - have our final minimal self-attention building
27:26 - block with which we can replace our RNNs.
27:29 - And that's that-- well, when I've been writing out
27:33 - all of these examples of self-attention,
27:35 - you can sort of look at the entire sequence, right?
27:38 - And in practice, for some tasks such as machine translation
27:42 - or language modeling, whenever you
27:44 - want to define a probability distribution over a sequence,
27:47 - you can't cheat and look at the future, right?
27:51 - So at every time step, I could define
27:54 - the set of keys and queries and values
27:57 - to only include past words but this is inefficient--
28:01 - bear with me.
28:02 - It's inefficient because you can't parallelize it so well.
28:04 - So instead, we compute the entire n by n matrix
28:08 - just like I showed in the slide discussing self-attention,
28:11 - and then I mask out words in the future.
28:13 - So this score eij-- right, and I computed eij for all n by n
28:18 - pairs of words--
28:20 - is equal to whatever it was before if the word that you're
28:25 - looking at index j is an index that
28:28 - is less than or equal to where you are,
28:30 - index i, and it's equal to negative infinity-ish
28:34 - otherwise if it's in the future.
28:36 - And when you softmax the eij, negative infinity
28:38 - gets mapped to 0.
28:41 - So now my attention is weighted 0-- my weighted average is
28:45 - 0 on the future.
28:46 - So I can't look at it.
28:48 - What does this look like?
28:50 - So in order to encode these words, "the chef who"
28:54 - and maybe the start symbol there,
28:58 - I can look at these words--
29:00 - that's all pairs of words, and then I just gray out.
29:03 - I sort of negative infinity out the words I can't look at.
29:06 - So encoding the start symbol, I can just
29:08 - look at the start symbol.
29:09 - When encoding "the", I can look at the start symbol and the.
29:13 - Encoding "chef", I can look at start "the chef"
29:16 - but I can't look at "who" right?
29:19 - And so with this representation of chef
29:21 - that encode that is only looking at start, the, chef,
29:26 - I can define a probability distribution
29:28 - using this vector that allows me to predict
29:30 - who without having cheated by already
29:32 - looking ahead and seeing that, well, "who" is the next word.
29:37 - The question is-- it says here that we're
29:39 - using this in a decoder.
29:40 - Do we also use it in the encoder?
29:42 - So that this is the distinction between sort
29:44 - of like a bidirectional LSTM and a unidirectional LSTM, right?
29:48 - So wherever you don't need this constraint,
29:52 - you probably don't use it.
29:53 - So if you're using an encoder, right,
29:55 - on the source sentence of your machine translation problem,
29:57 - you probably don't do this masking
29:59 - because it's probably good to let
30:01 - everything look at each other.
30:02 - And then whenever you do need to use it
30:04 - because you have this autoregressive sort
30:06 - of probability of word 1, probability of 2
30:08 - given 1, 3 given 2 and 1, then you would use this.
30:11 - So traditionally, yes, in decoders, you will use it.
30:14 - In encoders, you will not.
30:16 - So the question is, isn't looking ahead a little bit
30:19 - and sort of predicting or getting an idea of the words
30:21 - that you might say in the future sort of how humans generate
30:24 - language instead of the sort of strict constraint
30:27 - of not seeing into the future that.
30:28 - Is that what you're--
30:29 - OK.
30:30 - So right.
30:31 - Trying to plan ahead to see what I should do
30:35 - is definitely an interesting idea.
30:37 - But when I am training the network, right, I can't--
30:41 - if I'm teaching it to try to predict the next word
30:44 - and if I give it the answer, it's not
30:46 - going to learn anything useful.
30:48 - So in practice, when I'm generating text,
30:50 - maybe it would be a good idea to make some guesses far
30:53 - into the future or have a high-level plan or something.
30:56 - But in training the network, I can't encode that intuition
31:00 - about how humans build like generate sequences of language
31:04 - by just giving it the answer of the future directly
31:06 - at least because then it's just too easy, like there's
31:08 - nothing to learn.
31:10 - Yeah, but there might be interesting ideas
31:12 - about maybe giving the network like a hint as to what
31:15 - kind of thing could come next, for example,
31:17 - but that's out of scope for this.
31:19 - Yeah.
31:19 - So in machine translation, I have a sentence
31:22 - like "I like pizza" and I want to be able to translate it--
31:34 - J'aime la pizza.
31:38 - Nice, right?
31:39 - And so when I'm looking at the "I like pizza," right,
31:44 - I get this as the input.
31:45 - And so I want self-attention without masking
31:52 - because I want I to look at like, and I to look at pizza,
31:56 - and like to look at pizza, and I want it all.
31:58 - And then when I'm generating this, right,
32:00 - if my tokens are like J aime la pizza,
32:04 - I want to-- encoding this word--
32:07 - I want to be able to look only at myself,
32:10 - and we'll talk about encoder-decoder architectures
32:12 - in this later in the lecture, but I
32:15 - want to be able to look at myself none of the future
32:18 - and all of this.
32:19 - And so what I'm talking about right now in this masking case
32:22 - is masking out with like negative infinity
32:27 - all of these words.
32:28 - So that sort of attention score from J to everything else
32:32 - should be a net-- to be negative infinity.
32:35 - Yeah.
32:36 - So that was our last big sort of building block issue
32:40 - with self-attention.
32:41 - So this is what I would call, and this
32:42 - is my personal opinion, a minimal self-attention building
32:45 - block.
32:46 - You have self-attention the basis of the method.
32:49 - So that's sort of here in the red,
32:52 - and maybe we had the inputs to the sequence here,
32:55 - and then you embed it with that embedding matrix E.
32:57 - And then you add position embeddings, right?
33:00 - Then these three arrows represent
33:01 - using the key, the value, and the query, that's
33:06 - sort of stylized there.
33:07 - This is often how you see these diagrams, right?
33:10 - And so you pass it to self-attention
33:13 - with the position representation, right?
33:16 - So that specifies the sequence order
33:18 - because otherwise, you'd have no idea what
33:20 - order the words showed up in.
33:22 - You have the non-linearities in sort
33:24 - of the TL feedforward network there to sort of provide
33:28 - that sort of squashing and sort of deep learning expressivity.
33:32 - And then you have masking in order
33:34 - to have parallelizable operations that
33:36 - don't look at the future, OK?
33:38 - So this is sort of our minimal architecture.
33:41 - And then up at the top above here, right,
33:43 - so you have this thing.
33:44 - Maybe you repeat this sort of self-attention and feedforward
33:47 - many times.
33:47 - So self-attention, feedforward, self-attention, feedforward,
33:50 - self-attention, feedforward, right?
33:52 - That's what I'm calling this block.
33:54 - And then maybe at the end of it, you predict something.
33:56 - I don't know.
33:57 - We haven't really talked about that,
33:58 - but you know, have these representations,
34:00 - and then you predict the next word,
34:01 - or you predict the sentiment, or you predict whatever.
34:04 - So this is like a self-attention architecture.

*************************************************************************************


*******************************Transformers Models, Results, and Tradeoffs

00:00 -
00:04 - SPEAKER: So now let's talk about the transformer.
00:06 - So what I've pitched to you is what
00:09 - I call a minimal self-attention architecture.
00:13 - And I quite like pitching it that way.
00:18 - But really, no one uses the architecture
00:20 - that was just up on the slide, the previous slide.
00:23 - It doesn't work quite as well as it could,
00:25 - and there's a bunch of sort of important details
00:27 - that we'll talk about now that goes into the transformer.
00:30 - But what I would hope though to have you take away from that
00:35 - is that the transformer architecture,
00:37 - as I'll present it now, is not necessarily the end
00:41 - point of our search for better and better ways of representing
00:44 - language even though it's now ubiquitous and has
00:47 - been for a couple of years.
00:48 - So think about these ideas of the problems
00:51 - of using self-attention and maybe ways of fixing
00:55 - some of the issues with transformers.
00:58 - OK.
00:58 - So a transformer decoder is how we'll build systems
01:02 - like language models.
01:03 - And so we've discussed this.
01:04 - It's like our decoder with our self-attention only minimal
01:08 - architecture.
01:09 - It's got a couple of extra components, some of which
01:11 - I've grayed out here that we'll go over one by one.
01:14 - The first that's actually different
01:17 - is that we'll replace our self-attention
01:20 - with masking with masked multi-head self-attention.
01:24 - This ends up being crucial, it's probably
01:26 - the most important distinction between the transformer
01:29 - and this sort of minimal architecture
01:31 - that I've presented.
01:32 - So let's come back to our toy example of attention where
01:36 - we've been trying to represent the word learned
01:38 - in the context of the sequence, I went to Stanford CS224n
01:41 - and learned.
01:43 - And I was giving these teal bars to say, oh, maybe intuitively,
01:48 - you look at various things to build up
01:50 - your representation of learned.
01:53 - But really, there are varying ways
01:55 - in which I want to look back at the sequence
01:58 - to see varying sort of aspects of information
02:02 - that I want to incorporate into my representation.
02:05 - So maybe in this way, I want to look at Stanford CS224N
02:12 - because it's like entities like it you learn
02:15 - different stuff at Stanford CS224N
02:17 - than you do at other courses or other universities or whatever,
02:20 - right?
02:21 - And so maybe I want to look here for this reason, and maybe
02:25 - this--
02:25 - in another sense, I actually want
02:27 - to look at the word learned, and I want to look at I,
02:29 - and I went, and learned, maybe syntactically relevant words,
02:34 - right?
02:35 - It's very different reasons for which
02:37 - I might want to look at different things
02:38 - in the sequence.
02:40 - And so trying to average it all out with a single operation
02:43 - of self-attention ends up being maybe somewhat too difficult
02:47 - in a way that will make precise in assignment 5.
02:49 - Nice, we'll do a little bit more math.
02:51 - Yeah.
02:51 - So it should be an application of attention
02:54 - just as I've presented it, right?
02:56 - So one independent, define the keys,
02:58 - define the queries to find the values.
03:00 - I'll define it more precisely here.
03:01 - But think of it as, I do attention once and then
03:04 - I do it again with different--
03:07 - different parameters being able to look
03:09 - at different things, et cetera.
03:10 - So the question is, if we have two separate sets of weights
03:12 - trying to learn, say, to do this and to do that,
03:15 - how do we ensure that they learn different things?
03:17 - We do not ensure that they hope-- that they
03:19 - learn different things.
03:20 - And in practice, they do, although not perfectly.
03:23 - So it ends up being the case that you have some redundancy
03:26 - and you can cut out some of these,
03:28 - but we're out of scope for this.
03:30 - But we hope-- just like we hope that different dimensions
03:33 - in our feed-forward layers will learn different things because
03:36 - of lack of symmetry and whatever,
03:38 - we hope that the heads will start to specialize
03:40 - and that will mean they'll specialize even more and yeah.
03:44 - All right.
03:44 - So in order to discuss multi-head self-attention well,
03:47 - we really need to talk about the matrices.
03:49 - How we're going to implement this in GPUs efficiently.
03:53 - We're going to talk about the sequence stacked
03:55 - form of attention.
03:57 - So we've been talking about each word
03:58 - individually as a vector in dimensionality, d.
04:01 - But really, we're going to be working
04:03 - on these as big matrices that are stacked.
04:06 - So I take all of my word embeddings, X1 to Xn,
04:10 - and I stack them together, and now I
04:12 - have a big matrix that is in dimensionality Rn by d.
04:17 - OK.
04:18 - And now, with my matrices K, Q, and V, I can just multiply them
04:25 - on this side of X. So X is Rn by d, K is Rd by d.
04:30 - So n by d times d by d gives you n by d again.
04:34 - So I can just compute a big matrix multiply
04:38 - on my whole sequence to multiply each one
04:40 - of the words with my key query and value matrices
04:43 - very efficiently.
04:45 - So this is this vectorization idea.
04:46 - I don't want a for loop over the sequence.
04:48 - I represent the sequence as a big matrix,
04:51 - and I just do one big matrix multiply.
04:55 - Then the output is defined as this inscrutable bit
04:58 - of math, which I'm going to go over visually.
05:02 - So first, we're going to take the key query dot
05:05 - products in one matrix.
05:07 - So we've got Xq, which is Rn by d,
05:14 - and I've got XK transpose, which is Rd by n.
05:18 - So n by d, d by n.
05:21 - This is computing all of the EIJs, these scores
05:24 - for self-attention, right?
05:25 - So this is all pairs of attention scores computed
05:28 - in one big matrix multiply, OK?
05:34 - So this big matrix here.
05:36 - Next, I use the softmax.
05:39 - So I softmax this over the second dimension, the second n
05:44 - dimension, and I get my sort of normalized scores
05:48 - and then I multiply with XV.
05:50 - So this is an n by n matrix multiplied by an n by d matrix,
05:55 - and what do I get?
05:56 - Well, this is just doing the weighted average.
05:59 - So this is one big weighted average contribution
06:02 - on the whole matrix giving me my whole self-attention output
06:05 - and Rn by d.
06:07 - So I've just restated identically
06:09 - the self-attention operations, but computed
06:11 - in terms of matrices.
06:13 - So that you could do this efficiently on a GPU.
06:15 -
06:18 - OK.
06:19 - So multi-headed attention.
06:21 - This is going to give us-- and this it's
06:23 - going to be important to compute this
06:25 - in terms of the matrices, which we'll see.
06:27 - This is going to give us the ability
06:28 - to look in multiple places at once for different reasons.
06:31 - So for self attention looks where this dot product here
06:36 - is high, the Q matrix, the key matrix.
06:42 - But maybe we want to look in different places
06:45 - for different reasons.
06:47 - So we actually define multiple query key and value matrices.
06:52 - So I'm going to have a bunch of heads.
06:54 - I'm going to have h self attention heads.
06:57 - And for each head, I'm going to define an independent query key
07:01 - and value matrix, and I'm going to say that its shape is going
07:05 - to map from the model dimensionality
07:07 - to the model dimensionality over h.
07:09 - So each one of these is doing projection
07:10 - down to a lower dimensional space.
07:13 - This can be for computational efficiency,
07:15 - and I'll just apply self attention sort of independently
07:19 - for each output.
07:21 - So this equation here is identical to the one
07:23 - we saw for single headed self attention
07:26 - except I've got these L indices everywhere.
07:30 - So I've got this lower dimensional thing
07:32 - I'm mapping to a lower dimensional space,
07:34 - and then I do have my lower dimensional value vector there.
07:37 - So my output is an Rd by h.
07:39 - But really, you're doing exactly the same operation.
07:42 - I'm just doing it h different times.
07:45 - And then you combine the outputs.
07:47 - So I've done look in different places
07:49 - with the different key query and value matrices,
07:52 - and then I get each of their outputs,
07:55 - and then I concatenate them together.
07:58 - So each one is dimensionality d by h
08:01 - and I concatenate them together and then mix them together
08:04 - with the final linear transformation.
08:07 - And so each head gets to look at different things
08:10 - and construct their value vectors differently,
08:13 - and then sort of I combine the result all together at once.
08:16 - OK.
08:17 - Let's go through this visually because it's
08:19 - at least helpful for me.
08:21 - So, right, it's actually not more costly to do this really
08:26 - than it is to compute a single head of self-attention.
08:28 - In single headed self-attention, we computed XQ,
08:31 - and in multi-headed self-attention,
08:33 - we'll also compute XQ the same way.
08:36 - And then we can reshape it into Rn, that's sequence length,
08:40 - times the number of heads, times the model dimensionality
08:45 - over the number of heads.
08:46 - So I've just reshaped it to say, now
08:49 - I've got a big three axis tensor.
08:52 - The first axis is the sequence length
08:54 - the second one is the number of heads
08:55 - the third is this reduced model dimensionality.
08:58 - And that costs nothing.
09:00 - And do the same thing for X and V,
09:02 - and then I transpose so that I've got the head
09:05 - axis as the first axis.
09:07 - And now I can compute all my other operations
09:09 - with the head axis like a batch.
09:14 - So what does this look like in practice?
09:18 - Instead of having one big XQ matrix that's
09:21 - model dimensionality d, I've got, like in this case,
09:24 - three XQ matrices of model dimensionality d by 3, d by 3,
09:29 - and d by 3.
09:30 - Same thing with the key matrix here.
09:32 - So everything looks almost identical.
09:35 - It's just a reshaping of the tensors.
09:37 - And now, at the output of this, I've
09:39 - got three sets of attention scores,
09:42 - right, just by doing this reshape.
09:45 - And the cost is that, well, each of my attention heads
09:49 - has only a d by h vector to work with instead
09:52 - of a d dimensional vector to work with, right?
09:54 - So I get the output I get these three sets of pairs of scores.
09:59 - I compute the softmax independently
10:02 - for each of the three, and then I have three value matrices
10:06 - there as well, each of them lower dimensional.
10:09 - And then finally, I get my three different output vectors
10:13 - and I have a final linear transformation
10:15 - to sort of mush them together, and I get an output.
10:19 - And in summary, what this allows you
10:21 - to do is exactly what I gave in the toy example, which was,
10:25 - I can have each of these heads look
10:27 - at different parts of a sequence for different reasons.
10:30 - The question is, are all of these for a given block?
10:34 - And we'll talk about a block again,
10:35 - but this block was this pair of self-attention and feed-forward
10:39 - networks.
10:40 - So you do self-attention feed-forward, that's one block.
10:42 - Another block is another self-attention,
10:44 - another feed-forward.
10:45 - And the question is, are the parameters shared
10:47 - between the blocks or not?
10:48 - Generally, they are not shared.
10:50 - You'll have independent parameters at every block,
10:52 - although there are some exceptions.
10:54 - So the question is, do you have different numbers of heads
10:56 - across the different blocks or do
10:58 - you have the same number of heads across all blocks?
11:01 - The simplest thing is to just have
11:03 - it be the same everywhere, which is what people have done.
11:05 - I haven't yet found a good reason to vary it,
11:08 - but, well, it could be interesting.
11:10 - It's definitely the case that after training these networks,
11:14 - you can actually just totally zero out remove
11:17 - some of the attention heads.
11:19 - And I'd be curious to know if you could remove more
11:24 - or less, depending on the layer index, which might then say,
11:28 - oh, we should just have fewer.
11:29 - But again, it's not actually more expensive to have a bunch.
11:32 - So people tend to instead set the number of heads
11:35 - to be roughly so that you have a reasonable number of dimensions
11:40 - per head given the total model dimensionality d that you want.
11:44 - So for example, I might want at least 64 dimensions
11:48 - per head, which if d is 128, that tells me
11:52 - how many heads I'm going to have roughly.
11:54 - So people tend to scale the number of heads up
11:57 - with the model dimensionality.
11:59 - So the question is, by having these reduced XQ and XK
12:05 - matrices, this is a very low rank approximation.
12:09 - This little sliver and this little sliver
12:11 - defining this whole big matrix it's very low rank.
12:14 - Is that not bad in practice?
12:16 - No.
12:17 - I mean, again, it's the reason why
12:19 - we limit the number of heads depending on the model
12:22 - dimensionality because you want intuitively at least
12:26 - some number of dimensions.
12:27 - So 64 is sometimes done 128, something like that.
12:32 - But if you're not giving each head too much to do
12:34 - and it's got a simple job, you've got a lot of hats,
12:37 - this ends up being OK.
12:40 - All we really know is that empirically, it's way better
12:43 - to have more heads than one.
12:47 - So the question is, have there been
12:48 - studies to see if there's sort of consistent information
12:51 - encoded by the attention heads?
12:53 - And yes, actually, there's been quite a lot
12:56 - of sort of study in interpretability
12:58 - and analysis of these models to try to figure out what roles--
13:01 - what mechanistic roles each of these heads takes on.
13:04 - And there's quite a bit of exciting results there
13:08 - around some attention heads learning
13:10 - to pick out sort of the syntactic dependencies
13:14 - or maybe doing a sort of global averaging of context.
13:18 - The question is quite nuanced though because
13:20 - in a deep network, it's unclear--
13:22 - we should talk about this more offline,
13:24 - but it's unclear if you look at a word
13:26 - 10 layers deep in a network, what you're really
13:28 - looking at because it's already incorporated context
13:32 - from everyone else and it's a little bit unclear active area
13:35 - of research.
13:36 - But I think I should move on now to keep
13:40 - discussing transformers.
13:41 - OK.
13:42 - So another hack that I'm going to toss in here--
13:45 - I mean, maybe they wouldn't call it hack,
13:47 - but it's a nice little method to improve things.
13:50 - It's called scaled dot product attention.
13:53 - So one of the issues with this key query value self attention
13:57 - is that when the model dimensionality becomes large,
14:00 - the dot products between vectors, even random vectors,
14:02 - tend to become large.
14:06 - And when that happens, the inputs to the softmax function
14:09 - can be very large making the gradients small.
14:12 - So intuitively, if you have two random vectors and model
14:14 - dimensionality d and you just dot product
14:17 - them together, as d grows, their dot product
14:20 - grows in expectation to be very large.
14:22 - And so you sort of want to start out with everyone's attention
14:26 - being very uniform, very flat, look everywhere.
14:29 - But if some dot products are very large, then learning
14:33 - will be inhibited.
14:34 - And so what you end up doing is you just--
14:37 - for each of your heads, you just divide all the scores
14:40 - by this constant that's determined
14:42 - by the model dimensionality.
14:44 - So as the vectors grow very large,
14:46 - their dot products don't, at least at initialization time.
14:51 - So this is sort of like a nice little important but maybe
14:56 - not terribly-- yeah, it's important to know.
15:03 - And so that's called scaled dot product attention.
15:06 - From here on out, we'll just assume that we do this.
15:09 - It's quite easy to implement.
15:10 - You just do a little division in all of your computations.
15:16 - OK.
15:16 - So now in the transformer decoder,
15:18 - we've got a couple of other things
15:19 - that I have unfaded out here.
15:23 - We have two big optimization tricks or optimization methods,
15:26 - I should say really, because these are quite important.
15:28 - They end up being very important.
15:31 - We've got residual connections and layer normalization.
15:33 - And in transformer diagrams that you see around the web,
15:38 - they're often written together as this add and norm box.
15:43 - And in practice, in the transformer decoder,
15:45 - I'm going to apply masked multi-head attention,
15:49 - and then do this optimization add a norm.
15:52 - Then I'll do a feed-forward application
15:54 - and then add a norm.
15:55 - So this is quite important.
15:58 - So let's go over these two individual components.
16:02 - The first is residual connections.
16:04 - I mean, we've--
16:05 - I think we've talked about residual connections before,
16:06 - right?
16:07 - It's worth doing it again.
16:09 - But it's really a good trick to help models train better.
16:12 - So just to recap, we're going to take instead of having this--
16:17 - you have a layer I minus 1, and you pass it through a thing,
16:21 - maybe it's self-attention, maybe it's a feed-forward network.
16:24 - Now you've got layer I.
16:27 - I'm going to add the result of layer i to its input here.
16:34 - So now I'm saying I'm just going to compute the layer
16:36 - and I'm going to add in the input to the layer
16:38 - so that I only have to learn the residual
16:41 - from the previous layer, right?
16:43 - So I've got this connection here.
16:44 - It's often written as this sort of like "ooh" connection,
16:49 - OK, right?
16:50 - Goes around.
16:50 - And you should think that the gradient is just
16:52 - really great through the residual connection, right?
16:55 - Like ah, If I've got vanishing or exploding gradient
16:58 - then I use vanishing gradients through this layer, well,
17:00 - I can at least learn everything behind it
17:02 - because I've got this residual connection where the gradient
17:05 - is 1 because it's the identity.
17:09 - So this is really nice.
17:10 - And it also maybe is like-- at least at initialization,
17:14 - everything looks a little bit like the identity function now.
17:17 - Because if the contribution of the layer
17:20 - is somewhat small because all of your weights are small
17:22 - and I have the addition from the input,
17:24 - maybe the whole thing looks a little bit
17:26 - like the identity, which might be a good place to start.
17:31 - And there are really nice visualizations.
17:33 - I just love this visualization.
17:35 - So this is your loss landscape.
17:37 - So your gradient descent and you're
17:39 - trying to traverse the mountains of the loss landscape.
17:42 - This is like the parameter space and down is better
17:44 - in your loss function.
17:45 - And it's really hard, so you get stuck in some local optima
17:49 - and you can't sort of find your way to get out.
17:52 - And then this is with residual connections.
17:54 - I mean, come on, you just sort of walk down.
17:58 - I mean, it's not actually I guess really
18:00 - how it works all the time.
18:01 - So yeah, we've seen residual connections.
18:03 - We should move on to layer normalization.
18:05 - So layer norm is another thing to help your model
18:09 - train faster.
18:11 - And the intuitions around layer normalization
18:17 - and the empiricism of it working very well maybe
18:20 - aren't perfectly, let's say, connected.
18:23 - But you should imagine, I suppose,
18:28 - that we want to say there's variation within each layer.
18:32 - Things can get very big.
18:34 - Things can get very small.
18:35 - That's not actually informative because of variations
18:39 - between maybe the gradients or I've
18:44 - got weird things going on in my layers
18:46 - that I can't totally control.
18:48 - I haven't been able to make everything behave nicely
18:51 - where everything stays roughly the same norm.
18:53 - Maybe some things explode.
18:54 - Maybe some things shrink.
18:57 - And I want to cut down on sort of uninformative variation
19:02 - between layers.
19:03 - So I'm going to let x and be an individual word
19:06 - vector in the model.
19:08 - So this is like at a single index 1 vector.
19:11 - And what I'm going to try to do is just normalize it.
19:15 - Normalize it in the sense of it's got a bunch of variation,
19:18 - and I'm going to cut out on everything--
19:20 - I'm going to normalize it to unit, mean, and standard
19:23 - deviation.
19:24 - So I'm going to estimate the mean here across--
19:29 - so for all of the dimensions in the vector.
19:33 - So j equals 1 to the model dimensionality.
19:35 - I'm going to sum up the value.
19:36 - So I've got this one big word vector
19:38 - and I sum up all the values.
19:40 - Division by d here, right, that's the mean.
19:43 - I'm going to have my estimate of the standard deviation.
19:46 - Again, these should say estimates.
19:47 - This is my simple estimate of the standard deviation
19:50 - of the values within this one vector.
19:53 - And I'm just going to--
19:56 - and then possibly, I guess I can have learned parameters
20:00 - to try to scale back out in terms of multiplicatively
20:05 - and additively here.
20:07 - But that's optional.
20:08 - We're going to compute this standardization where
20:11 - I'm going to take my vector x, subtract out the mean,
20:14 - divide by the standard deviation plus this epsilon
20:16 - sort of constant.
20:17 - If there's not a lot of variation,
20:19 - I don't want things to explode.
20:20 - So I'm going to have this epsilon there
20:22 - that's close to 0.
20:24 - So this part here--
20:26 - x minus mu over square root sigma plus epsilon, is saying,
20:29 - take all the variation and normalize it
20:32 - to unit mean and standard deviation.
20:35 - And then maybe I want to sort of scale it, stretch it back out,
20:39 - and then maybe add an offset beta that I've learned.
20:43 - Although in practice, actually this part--
20:45 - and discussed this in the lecture notes.
20:47 - In practice, this part maybe isn't actually that important.
20:50 - But so layer normalization, yeah, you're--
20:53 - you can think of this as when I get the output of layer
20:57 - normalization, it's going to look nice and look
21:00 - similar to the next layer independent of what's
21:02 - gone on because it's going to be unit
21:04 - mean and standard deviation.
21:05 - So maybe that makes for a better thing
21:07 - to learn off of for the next layer.
21:09 - When I subtract the scalar mu from the vector x,
21:12 - I broadcast mu to dimensionality d and remove mu from all d.
21:19 - Yeah, good point.
21:20 - So the question is, if I have five words in the sequence,
21:23 - do I normalize by sort of aggregating
21:26 - the statistics to estimate mu and sigma across all the five
21:29 - words share their statistics or do it independently
21:32 - for each word?
21:33 - This is a great question, which, I think in all the papers
21:35 - that discuss transformers, is underspecified.
21:39 - You do not share across the five words, which
21:42 - is somewhat confusing to me.
21:44 - So each of the five words is done completely independently.
21:48 - You could have shared across the five words
21:50 - and said that my estimate of the statistics
21:52 - are just based on all five, but you do not.
21:57 - So similar question.
21:59 - The question is, if you have a batch of sequences,
22:03 - so just like we're doing batch based training,
22:06 - do you, for a single word--
22:08 - now, we don't share across the sequence index
22:10 - for sharing the statistics, but do you share across the batch.
22:13 - And the answer is no, you also do not share across the batch.
22:16 - In fact, layer normalization was sort
22:18 - invented as a replacement for batch normalization, which
22:21 - did just that.
22:22 - And the issue with batch normalization
22:24 - is that now your forward pass depends, in a way
22:27 - that you don't like, on examples that should be not related
22:31 - to your example.
22:32 - And so yeah, you don't share statistics across the batch.
22:35 - OK.
22:35 - So now we have our full transformer decoder
22:39 - and we have our blocks.
22:41 - So in this slightly grayed out thing here that
22:44 - says repeat for number of encoder or, sorry, decoder
22:48 - blocks, each block consists of--
22:51 - I pass it through self-attention.
22:53 - And then my add and norm--
22:55 - so I've got this residual connection here
22:57 - that goes around, and I've got the layer normalization there,
23:01 - and then a feed-forward layer, and then another add and norm.
23:06 - And so that set of four operations,
23:09 - I apply for some number times the number of blocks.
23:12 - So that whole thing is called a single block.
23:14 - And that's it.
23:15 - That's the transformer decoder as it is.
23:19 -
23:22 - Cool.
23:23 - So that's a whole architecture right there.
23:25 - We've solved things like needing to represent position,
23:27 - we've solved things like not being
23:30 - able to look into the future, we've
23:33 - solved a lot of different optimization problems.
23:35 - So the question is, how do these models
23:38 - handle variable length inputs?
23:40 - Yeah.
23:41 - So if you have--
23:45 - so the input to the GPU forward pass
23:50 - is going to be a constant length.
23:52 - So you're going to maybe pad to a constant length.
23:56 - And in order to not look at the future, the stuff that's
24:00 - sort of happening in the future, you
24:02 - can mask out the pad tokens just like the masking
24:05 - that we showed for not looking at the future in general.
24:08 - You can just say, set all of the attention weights to 0
24:12 - or the scores to negative infinity
24:14 - for all of the pad tokens.
24:16 - So you can set everything to this maximum length.
24:19 - Now in practice-- so the question was,
24:21 - do you set this length that you have everything
24:23 - be that maximum length?
24:24 - I mean, yes, often.
24:26 - Although, you can save computation
24:28 - by setting it to something smaller, and the math
24:32 - all still works out.
24:33 - You just have to code it properly so it can handle--
24:36 - you set everything-- instead of to n, you set it all to 5
24:39 - if everything is shorter than length 5
24:41 - and you save a lot of computation.
24:42 - All of the self-attention operations just work.
24:46 - So in the encoder, so the transformer encoder,
24:49 - is almost identical.
24:50 - But again, we want bidirectional context,
24:52 - and so we just don't do the masking.
24:55 - So I've got-- in my multi-head attention here,
24:57 - I've got no masking.
24:59 - And so it's that easy to make the model bidirectional, OK?
25:03 - So that's easy.
25:04 - So that's called the transformer encoder.
25:06 - It's almost identical but no masking.
25:08 - And then finally, we've got the transformer encoder
25:11 - decoder, which is actually how the transformer was originally
25:15 - presented in this paper, Attention is All You Need.
25:18 - And this is when we want to have a sort
25:20 - of bidirectional network.
25:21 - Here's the encoder.
25:22 - It takes in, say, my source sentence
25:24 - for machine translation.
25:25 - Its multi-headed attention is not masked,
25:28 - and I have a decoder to decode out my sentence.
25:33 - But you'll see that this is slightly more complicated.
25:35 - I have my masked multi-head self-attention
25:38 - just like I had before in my decoder,
25:40 - but now I have an extra operation which
25:43 - is called cross attention where I'm going to use my decoder
25:48 - vectors as my queries, but then I'll
25:52 - take the output of the encoder as my keys and values.
25:56 - So now, for every word in the decoder,
25:58 - I'm looking at all the possible words in the output of all
26:02 - of the blocks of the encoder.
26:04 - Yeah.
26:04 - The question is, how do you get the keys and values and queries
26:06 - out of this single collapsed output?
26:08 - Now, remember the output for each word is just
26:11 - this weighted average of the value vectors
26:13 - for the previous words, right?
26:15 - And then from that output, for the next layer,
26:18 - we apply a new key query and value transformation
26:21 - to each of them for the next layer of self-attention.
26:24 - Yeah, you apply the key matrix-- the query matrix to the output
26:27 - of whatever came before it.
26:29 - Yeah.
26:30 - And so just in a little bit of math,
26:32 - we have these vectors h1 through hn, I'm going to call them,
26:37 - that are the output of the encoder,
26:39 - and then I've got vectors that are the output of the decoder.
26:44 - So I've got these Zs I'm calling for the decoder,
26:47 - and then I simply define my keys and my values from the encoder
26:53 - vectors, these h's.
26:55 - So I take the h's, I apply a key matrix and a value matrix,
26:58 - and then I define the queries from my decoder.
27:02 - So my queries here.
27:03 - So this is why two of the arrows come from the encoder,
27:06 - and one of the arrows comes from the decoder.
27:08 - I've got my Z's here, get my queries, my keys and values,
27:12 - from the encoder.
27:16 - OK.
27:18 - So that is it.
27:20 - I've got a couple of minutes.
27:21 - I want to discuss some of the results of transformers,
27:24 - and I'm happy to answer more questions about transformers
27:27 - after class.
27:28 - So really, the original results of transformers,
27:32 - they had this big pitch for like, oh, look,
27:34 - you can do way more computation because of parallelization.
27:38 - They got great results in machine translation.
27:41 - So you had transformers doing quite well,
27:49 - although not like astoundingly better,
27:52 - than existing machine translation systems.
27:56 - But they were significantly more efficient to train, right?
27:58 - Because you don't have this parallelization problem,
28:01 - you could compute on much more data much faster
28:03 - and you could make use of faster GPUs much more.
28:08 - After that, there were things like document generation
28:11 - where you had this old standard of sequence
28:13 - to sequence models with LSTMS, and eventually, everything
28:17 - became transformers all the way down.
28:21 - Transformers also enabled this revolution
28:23 - into pretraining, which we'll go over in lecture next class.
28:28 - And the efficiency, the parallelizability
28:31 - allows you to compute on tons and tons of data.
28:34 - And so after a certain point on sort
28:36 - of standard large benchmarks, everything
28:39 - became transformer based.
28:41 - This ability to make use of lots and lots of data lots and lots
28:44 - of compute just put transformers head and shoulders above LSTMs
28:48 - in, let's say, almost every sort of modern advancement
28:52 - in natural language processing.
28:56 - There are many sort of drawbacks and variants to transformers.
29:00 - The clearest one that people have
29:02 - tried to work on quite a bit is this quadratic compute problem.
29:05 - So this all pairs of interactions, right,
29:08 - means that our total computation for each block
29:10 - grows quadratically with the sequence length.
29:12 - And in a student's question, we heard that, well,
29:16 - as the sequence length becomes long,
29:17 - if I want to process a whole Wikipedia
29:20 - article, a whole novel, that becomes quite unfeasible.
29:24 - And actually, that's a step backwards in some sense.
29:27 - Because for recurrent neural networks,
29:28 - it only grew linearly with the sequence length.
29:31 - Other things people have tried to work on
29:33 - are better position representations
29:36 - because the absolute index of a word
29:38 - is not really the best way maybe to represent
29:41 - its position in a sequence.
29:44 - And just to give you an intuition of quadratic sequence
29:46 - length, remember that we had this big matrix multiply here
29:49 - that resulted in this matrix of n by n.
29:53 - And computing this is like a big cost.
29:56 - It costs a lot of memory.
29:58 - And so there's been work--
30:00 - yeah.
30:00 - And so if you think of the model dimensionality,
30:02 - it's like 1,000.
30:03 - Although today, it gets much larger.
30:05 - Then for a short sequence, so n is roughly 30, maybe the--
30:10 - if you're computing n squared times d, 30 isn't so bad.
30:14 - But if you had something like 50,000,
30:18 - then n squared becomes huge and totally infeasible.
30:22 - So people have tried to sort of map things
30:25 - down to a lower dimensional space
30:26 - to get rid of the quadratic computation.
30:30 - But in practice-- I mean, as people
30:33 - have gone to things like GPT 3, ChatGPT,
30:36 - most of the computation doesn't show up in the self-attention.
30:39 - So people are wondering, is it even
30:41 - necessary to get rid of the self-attention operations
30:45 - quadratic constraint?
30:46 - So an open form of research, whether this is necessary.
30:51 - And then finally, there have been a ton of modifications
30:53 - to the transformer over the last 5, 4-ish years.
30:58 - And it turns out that the original transformer
31:01 - plus maybe a couple of modifications
31:04 - is pretty much the best thing there is still.
31:07 - There have been a couple of things
31:09 - that end up being important.
31:10 - Changing out the nonlinearities in the feed-forward network
31:13 - ends up being important, but it's sort of
31:17 - had lasting power so far.
31:18 - And so it's-- but I think it's ripe for people to come through
31:22 - and think about how to sort of improve it in various ways.

*************************************************************************************


I want to write a blog post about back propagation

read these:
https://docs.google.com/gview?url=https://github.com/scpd-proed/XCS224N-Handouts/raw/main/Neural%20Network%20Learning/Neural%20Networks%2C%20Backpropagation.pdf

https://docs.google.com/gview?url=https://github.com/scpd-proed/XCS224N-Handouts/raw/main/Neural%20Network%20Learning/Neural%20Networks%2C%20Backpropagation.pdf



https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b

https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf

https://cs231n.stanford.edu/handouts/derivatives.pdf

https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf

https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b

your tasks:
- for each doc you read provide a short summary, so i understand you read them
- prepare a plan how you want to structure the post
- after we agree on the structure you'll be requested to write the post
- for each paragraph/section you will be requested to add a picture/diagram etc taken from the material you read (we will mention the source)
- also search the web for other meaningful source about the topic
- any questions you have, ask before stating preparing the plan

now start reading, searching and preparing the plan
